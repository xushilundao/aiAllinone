{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xushilundao/aiAllinone/blob/main/%E2%80%9C%E4%BD%BF%E7%94%A8_LLaMA_Factory_%E5%BE%AE%E8%B0%83_Llama3_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 使用 LLaMA Factory 微调 Llama-3 中文对话模型\n",
        "\n",
        "请申请一个免费 T4 GPU 来运行该脚本\n",
        "\n",
        "项目主页: https://github.com/hiyouga/LLaMA-Factory\n"
      ],
      "metadata": {
        "id": "gf60HoT633NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安装 LLaMA Factory 依赖"
      ],
      "metadata": {
        "id": "rMp4rTWk4TKZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQDp0sXX3qE4"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers==0.0.25\n",
        "!pip install .[bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 检查 GPU 环境\n",
        "\n",
        "免费 T4 申请教程：https://zhuanlan.zhihu.com/p/642542618"
      ],
      "metadata": {
        "id": "Gs68aSwm5MFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"需要 GPU 环境，申请教程：https://zhuanlan.zhihu.com/p/642542618\")"
      ],
      "metadata": {
        "id": "P6rwbyFa5LkF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 更新自我认知数据集\n",
        "\n",
        "可以自由修改 NAME 和 AUTHOR 变量的内容。"
      ],
      "metadata": {
        "id": "gYEF3SGI6XdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-Chinese\"\n",
        "AUTHOR = \"AI首席科学家林\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "7gX4PskL6UJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5e98dd-58e7-4feb-84b2-c43aa51fdd4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用 LLaMA Board Web UI 微调模型"
      ],
      "metadata": {
        "id": "cA7vWZ6om3cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "YIfzFgLsm2kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用命令行微调模型\n",
        "\n",
        "微调过程大约需要 30 分钟。"
      ],
      "metadata": {
        "id": "B6ap81295trx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # 进行指令监督微调\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
        "  dataset=\"identity,alpaca_gpt4_en,alpaca_gpt4_zh\",      # 使用 alpaca 和自我认知数据集\n",
        "  template=\"llama3\",                     # 使用 llama3 提示词模板\n",
        "  finetuning_type=\"lora\",                   # 使用 LoRA 适配器来节省显存\n",
        "  lora_target=\"all\",                     # 添加 LoRA 适配器至全部线性层\n",
        "  output_dir=\"llama3_lora\",                  # 保存 LoRA 适配器的路径\n",
        "  per_device_train_batch_size=2,               # 批处理大小\n",
        "  gradient_accumulation_steps=4,               # 梯度累积步数\n",
        "  lr_scheduler_type=\"cosine\",                 # 使用余弦学习率退火算法\n",
        "  logging_steps=10,                      # 每 10 步输出一个记录\n",
        "  warmup_ratio=0.1,                      # 使用预热学习率\n",
        "  save_steps=1000,                      # 每 1000 步保存一个检查点\n",
        "  learning_rate=5e-5,                     # 学习率大小\n",
        "  num_train_epochs=3.0,                    # 训练轮数\n",
        "  max_samples=300,                      # 使用每个数据集中的 300 条样本\n",
        "  max_grad_norm=1.0,                     # 将梯度范数裁剪至 1.0\n",
        "  quantization_bit=4,                     # 使用 4 比特 QLoRA\n",
        "  loraplus_lr_ratio=16.0,                   # 使用 LoRA+ 算法并设置 lambda=16.0\n",
        "  use_unsloth=True,                      # 使用 UnslothAI 的 LoRA 优化来获得两倍的训练速度\n",
        "  fp16=True,                         # 使用 float16 混合精度训练\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psywJyo75vt6",
        "outputId": "5c09bfdd-a14d-4684-bc6b-f1b50c70e667"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-05-08 02:00:25.252524: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-08 02:00:25.252579: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-08 02:00:25.253881: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-08 02:00:26.438826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "05/08/2024 02:00:31 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "05/08/2024 02:00:31 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 51.1k/51.1k [00:00<00:00, 5.46MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 48.6MB/s]\n",
            "special_tokens_map.json: 100% 464/464 [00:00<00:00, 3.79MB/s]\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:00:31,942 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:00:31,942 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:00:31,942 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:00:31,942 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-05-08 02:00:32,316 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "05/08/2024 02:00:32 - INFO - llmtuner.data.template - Replace eos token: <|eot_id|>\n",
            "05/08/2024 02:00:32 - INFO - llmtuner.data.loader - Loading dataset identity.json...\n",
            "05/08/2024 02:00:32 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/identity.json.\n",
            "Generating train split: 91 examples [00:00, 5260.22 examples/s]\n",
            "Converting format of dataset: 100% 91/91 [00:00<00:00, 7749.41 examples/s]\n",
            "05/08/2024 02:00:32 - INFO - llmtuner.data.loader - Loading dataset alpaca_gpt4_data_en.json...\n",
            "Generating train split: 52002 examples [00:00, 78521.74 examples/s]\n",
            "Converting format of dataset: 100% 300/300 [00:00<00:00, 32201.95 examples/s]\n",
            "05/08/2024 02:00:33 - INFO - llmtuner.data.loader - Loading dataset alpaca_gpt4_data_zh.json...\n",
            "Generating train split: 48818 examples [00:00, 94992.62 examples/s]\n",
            "Converting format of dataset: 100% 300/300 [00:00<00:00, 31631.25 examples/s]\n",
            "Running tokenizer on dataset: 100% 691/691 [00:00<00:00, 1609.05 examples/s]\n",
            "input_ids:\n",
            "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 445, 81101, 30653, 7496, 11, 459, 15592, 18328, 8040, 555, 15592, 61075, 105350, 108396, 46729, 102138, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am Llama-Chinese, an AI assistant developed by AI首席科学家林. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 445, 81101, 30653, 7496, 11, 459, 15592, 18328, 8040, 555, 15592, 61075, 105350, 108396, 46729, 102138, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am Llama-Chinese, an AI assistant developed by AI首席科学家林. How can I assist you today?<|eot_id|>\n",
            "config.json: 100% 1.15k/1.15k [00:00<00:00, 8.61MB/s]\n",
            "[INFO|configuration_utils.py:726] 2024-05-08 02:00:35,357 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-08 02:00:35,358 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "05/08/2024 02:00:35 - INFO - llmtuner.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:726] 2024-05-08 02:00:35,590 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-08 02:00:35,591 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "[INFO|configuration_utils.py:726] 2024-05-08 02:00:35,666 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-08 02:00:35,667 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:726] 2024-05-08 02:00:35,781 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-08 02:00:35,782 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[WARNING|quantization_config.py:282] 2024-05-08 02:00:35,903 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "model.safetensors: 100% 5.70G/5.70G [00:55<00:00, 103MB/s] \n",
            "[INFO|modeling_utils.py:3429] 2024-05-08 02:01:31,693 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/model.safetensors\n",
            "[INFO|modeling_utils.py:1494] 2024-05-08 02:01:31,837 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:928] 2024-05-08 02:01:31,844 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4170] 2024-05-08 02:01:56,636 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4178] 2024-05-08 02:01:56,636 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 131/131 [00:00<00:00, 1.02MB/s]\n",
            "[INFO|configuration_utils.py:883] 2024-05-08 02:01:56,766 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/generation_config.json\n",
            "[INFO|configuration_utils.py:928] 2024-05-08 02:01:56,766 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 51.1k/51.1k [00:00<00:00, 3.12MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 43.2MB/s]\n",
            "special_tokens_map.json: 100% 464/464 [00:00<00:00, 3.43MB/s]\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:01:57,882 >> loading file tokenizer.json from cache at huggingface_tokenizers_cache/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:01:57,882 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:01:57,882 >> loading file special_tokens_map.json from cache at huggingface_tokenizers_cache/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:01:57,882 >> loading file tokenizer_config.json from cache at huggingface_tokenizers_cache/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-05-08 02:01:58,263 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:01:58,353 >> loading file tokenizer.json from cache at huggingface_tokenizers_cache/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:01:58,353 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:01:58,353 >> loading file special_tokens_map.json from cache at huggingface_tokenizers_cache/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:01:58,353 >> loading file tokenizer_config.json from cache at huggingface_tokenizers_cache/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-05-08 02:01:58,693 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "05/08/2024 02:01:59 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.\n",
            "05/08/2024 02:01:59 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "05/08/2024 02:01:59 - INFO - llmtuner.model.utils.misc - Found linear modules: v_proj,o_proj,down_proj,q_proj,k_proj,up_proj,gate_proj\n",
            "[WARNING|logging.py:329] 2024-05-08 02:02:00,051 >> Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
            "05/08/2024 02:02:00 - INFO - llmtuner.model.loader - trainable params: 20971520 || all params: 8051232768 || trainable%: 0.2605\n",
            "[INFO|trainer.py:626] 2024-05-08 02:02:00,126 >> Using auto half precision backend\n",
            "05/08/2024 02:02:00 - INFO - llmtuner.train.utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[WARNING|logging.py:329] 2024-05-08 02:02:00,471 >> ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 691 | Num Epochs = 3\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 258\n",
            " \"-____-\"     Number of trainable parameters = 20,971,520\n",
            "{'loss': 1.4995, 'grad_norm': 1.0740561485290527, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.12}\n",
            "{'loss': 1.2946, 'grad_norm': 0.5050432682037354, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.23}\n",
            "{'loss': 1.1914, 'grad_norm': 0.7335019707679749, 'learning_rate': 4.996333534627809e-05, 'epoch': 0.35}\n",
            "{'loss': 1.2194, 'grad_norm': 0.9669033288955688, 'learning_rate': 4.95520920685539e-05, 'epoch': 0.46}\n",
            "{'loss': 1.1613, 'grad_norm': 1.0680584907531738, 'learning_rate': 4.869132927957007e-05, 'epoch': 0.58}\n",
            "{'loss': 1.2805, 'grad_norm': 0.910470724105835, 'learning_rate': 4.73968065189672e-05, 'epoch': 0.69}\n",
            "{'loss': 1.2042, 'grad_norm': 0.6718502044677734, 'learning_rate': 4.5692224953922266e-05, 'epoch': 0.81}\n",
            "{'loss': 1.0702, 'grad_norm': 0.9948371648788452, 'learning_rate': 4.360879343905676e-05, 'epoch': 0.92}\n",
            "{'loss': 1.0855, 'grad_norm': 1.1975197792053223, 'learning_rate': 4.118465711954569e-05, 'epoch': 1.04}\n",
            "{'loss': 0.7818, 'grad_norm': 0.8067037463188171, 'learning_rate': 3.8464199039022605e-05, 'epoch': 1.16}\n",
            "{'loss': 0.9283, 'grad_norm': 0.7781421542167664, 'learning_rate': 3.5497227539006614e-05, 'epoch': 1.27}\n",
            "{'loss': 0.8355, 'grad_norm': 1.9114394187927246, 'learning_rate': 3.233806432759837e-05, 'epoch': 1.39}\n",
            "{'loss': 0.8645, 'grad_norm': 0.6831575632095337, 'learning_rate': 2.9044549913819124e-05, 'epoch': 1.5}\n",
            "{'loss': 0.858, 'grad_norm': 0.9479559063911438, 'learning_rate': 2.5676984616903367e-05, 'epoch': 1.62}\n",
            "{'loss': 0.8021, 'grad_norm': 1.033067226409912, 'learning_rate': 2.2297024539401463e-05, 'epoch': 1.73}\n",
            "{'loss': 0.7841, 'grad_norm': 1.0005431175231934, 'learning_rate': 1.8966552717507364e-05, 'epoch': 1.85}\n",
            "{'loss': 0.7894, 'grad_norm': 0.9219709038734436, 'learning_rate': 1.574654611650214e-05, 'epoch': 1.97}\n",
            "{'loss': 0.6037, 'grad_norm': 0.691892147064209, 'learning_rate': 1.2695959215274816e-05, 'epoch': 2.08}\n",
            "{'loss': 0.5095, 'grad_norm': 1.2133337259292603, 'learning_rate': 9.870644620155877e-06, 'epoch': 2.2}\n",
            "{'loss': 0.54, 'grad_norm': 1.0232725143432617, 'learning_rate': 7.3223304703363135e-06, 'epoch': 2.31}\n",
            "{'loss': 0.494, 'grad_norm': 0.9394548535346985, 'learning_rate': 5.097673357358907e-06, 'epoch': 2.43}\n",
            "{'loss': 0.5755, 'grad_norm': 0.991406261920929, 'learning_rate': 3.2374040985957004e-06, 'epoch': 2.54}\n",
            "{'loss': 0.5069, 'grad_norm': 1.2324331998825073, 'learning_rate': 1.7755820045802145e-06, 'epoch': 2.66}\n",
            "{'loss': 0.6074, 'grad_norm': 1.1073577404022217, 'learning_rate': 7.389712936697129e-07, 'epoch': 2.77}\n",
            "{'loss': 0.5259, 'grad_norm': 0.9226711988449097, 'learning_rate': 1.4655107114101007e-07, 'epoch': 2.89}\n",
            "100% 258/258 [28:10<00:00,  7.67s/it][INFO|<string>:474] 2024-05-08 02:30:11,196 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1690.7249, 'train_samples_per_second': 1.226, 'train_steps_per_second': 0.153, 'train_loss': 0.8718504868736563, 'epoch': 2.98}\n",
            "100% 258/258 [28:10<00:00,  6.55s/it]\n",
            "[INFO|trainer.py:3305] 2024-05-08 02:30:11,199 >> Saving model checkpoint to llama3_lora\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:726] 2024-05-08 02:30:11,437 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-08 02:30:11,438 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-05-08 02:30:11,657 >> tokenizer config file saved in llama3_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-05-08 02:30:11,657 >> Special tokens file saved in llama3_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     2.9827\n",
            "  total_flos               = 18618918GF\n",
            "  train_loss               =     0.8719\n",
            "  train_runtime            = 0:28:10.72\n",
            "  train_samples_per_second =      1.226\n",
            "  train_steps_per_second   =      0.153\n",
            "[INFO|modelcard.py:450] 2024-05-08 02:30:11,827 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型推理"
      ],
      "metadata": {
        "id": "otpDQuzaMBpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llmtuner.chat import ChatModel\n",
        "from llmtuner.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
        "  adapter_name_or_path=\"llama3_lora\",            # 加载之前保存的 LoRA 适配器\n",
        "  template=\"llama3\",                     # 和训练保持一致\n",
        "  finetuning_type=\"lora\",                  # 和训练保持一致\n",
        "  quantization_bit=4,                    # 加载 4 比特量化模型\n",
        "  use_unsloth=True,                     # 使用 UnslothAI 的 LoRA 优化来获得两倍的推理速度\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"使用 `clear` 清除对话历史，使用 `exit` 退出程序。\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"对话历史已清除\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbFsAE-y5so4",
        "outputId": "fb25ef7d-e78f-4217-d1ee-9d42a594278f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:30:21,335 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:30:21,337 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:30:21,343 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-08 02:30:21,345 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-05-08 02:30:22,137 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/08/2024 02:30:22 - INFO - llmtuner.data.template - Replace eos token: <|eot_id|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llmtuner.data.template:Replace eos token: <|eot_id|>\n",
            "[INFO|configuration_utils.py:726] 2024-05-08 02:30:22,204 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-08 02:30:22,207 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/08/2024 02:30:22 - INFO - llmtuner.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llmtuner.model.utils.quantization:Loading ?-bit BITSANDBYTES-quantized model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/08/2024 02:30:22 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llmtuner.model.patcher:Using KV cache for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/08/2024 02:30:22 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llmtuner.model.adapter:Fine-tuning method: LoRA\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:726] 2024-05-08 02:30:22,512 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-08 02:30:22,514 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:726] 2024-05-08 02:30:22,572 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-08 02:30:22,575 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:726] 2024-05-08 02:30:22,637 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-08 02:30:22,639 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING|quantization_config.py:282] 2024-05-08 02:30:22,765 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|modeling_utils.py:3429] 2024-05-08 02:30:22,768 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/model.safetensors\n",
            "[INFO|modeling_utils.py:1494] 2024-05-08 02:30:22,816 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:928] 2024-05-08 02:30:22,823 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4170] 2024-05-08 02:30:42,524 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4178] 2024-05-08 02:30:42,536 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:883] 2024-05-08 02:30:42,595 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/generation_config.json\n",
            "[INFO|configuration_utils.py:928] 2024-05-08 02:30:42,596 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-08 02:30:42,867 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-08 02:30:42,869 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-08 02:30:42,871 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-08 02:30:42,873 >> loading file tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-05-08 02:30:43,270 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-08 02:30:43,274 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-08 02:30:43,275 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-08 02:30:43,278 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-08 02:30:43,280 >> loading file tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-05-08 02:30:43,690 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|logging.py:329] 2024-05-08 02:30:45,496 >> Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/08/2024 02:30:45 - INFO - llmtuner.model.adapter - Loaded adapter(s): llama3_lora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llmtuner.model.adapter:Loaded adapter(s): llama3_lora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/08/2024 02:30:45 - INFO - llmtuner.model.loader - all params: 8051232768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llmtuner.model.loader:all params: 8051232768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用 `clear` 清除对话历史，使用 `exit` 退出程序。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并 LoRA 权重并上传模型\n",
        "\n",
        "注意：Colab 免费版仅提供了 12GB 系统内存，而合并 8B 模型的 LoRA 权重需要至少 18GB 系统内存，因此你 **无法** 在免费版运行此功能。"
      ],
      "metadata": {
        "id": "_7g7kprbhXo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "XA2kyAz-hXbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # 使用非量化的官方 Llama-3-8B-Instruct 模型\n",
        "  adapter_name_or_path=\"llama3_lora\",            # 加载之前保存的 LoRA 适配器\n",
        "  template=\"llama3\",                     # 和训练保持一致\n",
        "  finetuning_type=\"lora\",                  # 和训练保持一致\n",
        "  export_dir=\"llama3_lora_merged\",              # 合并后模型的保存目录\n",
        "  export_size=2,                       # 合并后模型每个权重文件的大小（单位：GB）\n",
        "  export_device=\"cpu\",                    # 合并模型使用的设备：`cpu` 或 `cuda`\n",
        "  #export_hub_model_id=\"your_id/your_model\",         # 用于上传模型的 HuggingFace 模型 ID\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "id": "eERYoAOrhpcu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}